<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:16px;
		margin-left: auto;
		margin-right: auto;
		width: 800px;
	}
	
	h1 {
		font-weight:300;
	}
		
	h2 {
		font-weight:300;
		font-size: 22px;
		text-align: left;
	}

	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	pre {
    text-align: left;
    white-space: pre;
	background-color: ghostwhite;
	border: 1px solid #CCCCCC;
	padding: 10px 20px;
	margin: 10px;
    tab-size:         4; /* Chrome 21+, Safari 6.1+, Opera 15+ */
    -moz-tab-size:    4; /* Firefox 4+ */
    -o-tab-size:      4; /* Opera 11.5 & 12.1 only */
  	}

</style>

<html>
  <head>
		<title>Parsing is All You Need for Accurate Gait Recognition in the Wild</title>
		<meta property="og:image" content=""/>
		<meta property="og:title" content="Parsing is All You Need for Accurate Gait Recognition in the Wild" />
<!--	  	<style>-->
<!--			p {text-indent:1em;}-->
<!--	 	</style>-->
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">Parsing is All You Need for Accurate Gait Recognition in the Wild</span>
	  		  <table align=center width=850px>
	  			  <tr>
	  	              <td align=center width=170px>
	  					<center>
	  						<span style="font-size:20px"><a href="http://jinkaizheng.com/">Jinkai Zheng<sup>1,2</sup></a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:20px"><a href="https://xinchenliu.com/">Xinchen Liu<sup>2</sup></a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=170px>
	  					<center>
	  						<span style="font-size:20px"><a>Shuai Wang<sup>1,3</sup></a></span>
		  		  		</center>
		  		  	  </td>
					<td align=center width=140px>
						<center>
							<span style="font-size:20px"><a>Lihao Wang<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=170px>
						<center>
							<span style="font-size:20px"><a href="https://cgyan-iipl.github.io/">Chenggang Yan<sup>1</sup></a></span>
						</center>
					</td>

					<td align=center width=100px>
						<center>
							<span style="font-size:20px"><a href="http://liuwu.weebly.com/">Wu Liu<sup>2</sup></a></span>
						</center>
					</td>
				</table>
          		<!-- <span style="font-size:30px">ICCV 2021.</span> -->

			  <table align=center width=850px>
				  <tr>
					  <td align=center width=300px>
						<center>
							<span style="font-size:18px"><a href="https://www.hdu.edu.cn/"><sup>1</sup>Hangzhou Dianzi University</a></span>
						</center>
					  </td>
					  <td align=center width=200px>
						<center>
							<span style="font-size:18px"><a href="https://www.jd.com/"><sup>2</sup>JD Explore Academy</a></span>
						</center>
					  </td>
					  <td align=center width=450px>
						<center>
							<span style="font-size:18px"><a href="http://hdlsyjywz.com/"><sup>3</sup>Lishui Institute of Hangzhou Dianzi University</a></span>
						</center>
 					  </td>

			  </table>
			  <a href="https://www.acmmm2023.org/" target="_blank">ACM International Conference on Multimedia (ACM MM) 2023</a>
<!--			IEEE Conference on Computer Vision and Pattern Recognition (<a href="http://cvpr2022.thecvf.com/" target="_blank">CVPR</a>) 2022, <font color="#e86e14">XXX Presentation</font>-->
<!--			TBDTBDTBDTBDTBDTBDTBDTBDTBDTBDTBDTBDTBD (<a href="https://github.com/" target="_blank">TBD</a>) 2022, <font color="#e86e14">TBDTBDTBDTBDTBD</font>-->
          </center>

   		  <br><br>
<!--		  <hr>-->

<!--  		  <br>-->
<!--  		  <table align=center width=720px>-->
<!--  			  <tr>-->
<!--  	              <td width=400px>-->
<!--  					<center>-->
<!--  	                	<a href="img/1.png" ><img class="rounded" src = "img/1.png" width="80%" ></img></href></a><br>-->
<!--					</center>-->
<!--  	              </td>-->
<!--                </tr>-->
<!--  	              <td width=400px>-->
<!--  					<center>-->
<!--  	                	<span style="font-size:14px"><i> Different gait representations of the same person from two viewpoints. Compared with silhouettes and skeletons, 3D meshes retain the shapes and viewpoints of the human body in the 3D space. (Best viewed in color.)</i>-->
<!--					</center>-->
<!--  	              </td>-->

<!--  		  </table>-->
<!--      	  <br><br>-->

		  <table align=center width=720px>
			<!-- <center><h1>Download</h1></center> -->
			<tr>
				<td width=300px>
					<center>
						<a href="#dataset"><img class="rounded" onmouseover="this.src='./resources/images/data_icon.png';" onmouseout="this.src='./resources/images/data_icon.png';" src = "./resources/images/data_icon.png" height = "120px"></a><br>
						<span style="font-size:16px">DataSet</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#method"><img class="rounded" onmouseover="this.src='./resources/images/idea.png';" onmouseout="this.src='./resources/images/idea.png';" src = "./resources/images/idea.png" height = "120px"></a><br>
						<span style="font-size:16px">Method</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
						<a href="#analysis"><img class="rounded" onmouseover="this.src='./resources/images/magnify_glass.png';" onmouseout="this.src='./resources/images/magnify_glass.png';" src = "./resources/images/magnify_glass.png" height = "120px"></a><br>
						<span style="font-size:16px">Analysis</span><br>
					</center>
				</td>
				<td width=300px>
					<center>
					  <a href="https://github.com/Gait3D/Gait3D-Benchmark"><img class="rounded" onmouseover="this.src='./resources/images/github_icon.png';" onmouseout="this.src='./resources/images/github_icon.png';" src = "./resources/images/github_icon.png" height = "120px"></a><br>
					  <span style="font-size:16px">GitHub Repo</span><br>						
					</center>
				</td>
			</tr>
		  </table>

		  <br><br>

		  <hr>

  		  <table align=center width=720px>
				<center><h1>Abstract</h1></center>
		  </table>
		  <p style="text-align:justify">
			<span>
				Binary silhouettes and keypoint-based skeletons have dominated
				human gait recognition studies for decades since they are easy to
				extract from video frames. Despite their success in gait recognition
				for in-the-lab environments, they usually fail in real-world scenarios 
				due to their low information entropy for gait representations.
				To achieve accurate gait recognition in the wild, this paper presents
				a novel gait representation, named <strong>Gait Parsing Sequence (GPS)</strong>.
				GPSs are sequences of fine-grained human segmentation, i.e., human parsing, 
				extracted from video frames, so they have much higher
				information entropy to encode the shapes and dynamics of finegrained human parts 
				during walking. Moreover, to effectively explore the capability of the GPS representation, 
				we propose a novel human parsing-based gait recognition framework, named <strong>ParsingGait</strong>. 
				ParsingGait contains a Convolutional Neural Network (CNN)-based backbone and two light-weighted heads. 
				The first head extracts global semantic features from GPSs, while the other
				one learns mutual information of part-level features through Graph
				Convolutional Networks to model the detailed dynamics of human walking. 
				Furthermore, due to the lack of suitable datasets, we build the first parsing-based dataset for gait recognition in the wild,
				named <strong>Gait3D-Parsing</strong>, by extending the large-scale and challenging Gait3D dataset. 
				Based on Gait3D-Parsing, we comprehensively evaluate our method and existing gait recognition methods. 
				Specifically, ParsingGait achieves a 17.5% Rank-1 increase compared with the state-of-the-art silhouette-based method. 
				In addition, by replacing silhouettes with GPSs, current gait recognition methods achieve about 12.5% ∼ 19.2% improvements 
				in Rank-1 accuracy. The experimental results show a significant improvement in accuracy brought
				by the GPS representation and the superiority of ParsingGait.
		  	</span>
		  </p>
  		  <br><br>
		  <hr>

		

<!--		  <table align=center width=720px>-->
<!--			<center><h1>2-Minute presentation video</h1></center>-->
<!--			<tr>-->
<!--				<table align=center width=720px>-->
<!--					<tr>-->
<!--						<td align=center width=720px>-->
<!--							<iframe width="600" height="320" src="https://www.youtube.com/watch?v=wJxa4aED6Kc" frameborder="0" allowfullscreen></iframe> -->
<!--						</td>-->
<!--					  </tr>-->
<!--					<tr>-->
<!--					 </table>-->
<!--			  </tr>-->
<!--		  </table>-->
<!--		   <br><br>-->
<!--		  <hr>-->


			<table align=center width=720px>
			  <center><h1 id="dataset">Gait3D-Parsing Dataset</h1></center>
				<table width=720px>
				<center><h2>(1) Examples of gait representations in the Gait3D-Parsing dataset.</h2></center>
				<tr width=720px>
					<td width=720px>
					  <center width=720px>
						  <a><img class="rounded"  style="display:block;margin-left:auto; margin-right:auto;width:780px"  src = "./resources/images/examples.png"></img></a><br>
<!--						  <a><img class="rounded" src = "./resources/images/figure_Gait3D_examples.png" width="500px"></img></a><br>-->
					</center>
					</td>
				</tr>
<!--					<td width=400px>-->
<!--					  <center>-->
<!--						  <span style="font-size:14px"><i>-->
<!--							Examples of gait representations in the Gait3D dataset.The sizes are normalized for visualization. (Best viewed in color.)-->
<!--							</i>-->
<!--					</center>-->
<!--					</td>-->

		  </table>
	      <br><br>
		  <hr>

		  <table align=center width=720px>
			<center><h2>(2) Statistics about the Gait3D-Parsing dataset.</h2></center>
			<tr>
<!--				<center>-->
<!--				<span>-->
<!--					Statistics about the Gait3D dataset.-->
<!--				</span>-->
<!--				<br>-->
<!--				</center>-->
				<td width=250px>
					<center>
						<span style="font-size:14px"><a>Frame # over class number #</a></span><br>
						<a><img class="rounded" src = "./resources/images/statistics_1.png" width="250px"></img></a><br>
					</center>
				</td>
				<td width=250px>
					<center>
						<span style="font-size:14px"><a>Frame # over class #</a></span><br>
						<a><img class="rounded" src = "./resources/images/statistics_2.png" width="250px"></img></a><br>
					</center>
				</td>
				<td width=250px>
					<center>
						<span style="font-size:14px"><a>Average proportion # over class #</a></span><br>
						<a><img class="rounded" src = "./resources/images/statistics_3.png" width="250px"></img></a><br>
					</center>
				</td>
			</tr>
		  </table>

		  <br><br>
		  <hr>


		  <table align=center width=720px>
			<center><h2>(3) Download Gait3D-Parsing.</h2></center>
			<tr>

				<td width=300px>
					<center>
						<span style="width:auto; display:block; text-align:left;font-size:18px">
							All users can obtain and use this dataset only after signing the
							<a href="./resources/AgreementForGait3D-Parsing.pdf" target="_blabk" style="color: #09f;">Agreement</a>
							and sending it to the official contact email address (gait3d.dataset@gmail.com).</span><br><br>
<!--						<img class="rounded" onmouseover="this.src='./resources/images/dataset_icon.jpg';" onmouseout="this.src='./resources/images/dataset_icon.jpg';" src = "./resources/images/dataset_icon.jpg" height = "150px"><br><br>-->
<!--						<span style="font-size:16px"><a href='resources/dataset/'>TBD</a></span><br>-->
					<span style="font-size:16px"></span>
					</center>
				</td>
				<table>
<!--				<center><h2> Updates </h2></center>-->
<!--&lt;!&ndash;				[01/10/2021] We include new subsections to track updates and address FAQs.<br>&ndash;&gt;-->
<!--				TBD.<br>-->
<!--				<center><h2> FAQs </h2></center>-->
<!--				Q0: TBD:<br>-->
<!--				A0: TBD.<br>-->
				 </table>

		 <br><br>
		 <hr>


		  <table align=center width=720px>
			<center><h1 id="method" >The Architecture of ParsingGait</h1></center>
			<tr>
				<td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/framework.png" width="780px"></img></a><br>
				</center>
				</td>
			</tr>
				<td width=400px>
				  <center>
					  <span style="font-size:14px"><i>
						The architecture of the ParsingGait framework for gait recognition in the wild.
						</i>
				</center>
				</td>

		  </table>
	      <br><br>
		  <hr>


		  <table align=center width=720px>
			<center><h1 id="analysis">Evaluation and Visualization</h1></center>
			<table>
			<center><h2>(1) Comparison of the state-of-the-art gait recognition methods.</h2></center>
			<tr>
				  <td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/experiment_sota.png" width="450px"></img></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align=center width=720px>
				  <span style="font-size:14px; text-align: justify;">
					  <i>
						  Comparison of the state-of-the-art gait recognition methods. 
						  With parsing, the performance of all methods was greatly improved (12.5% ~ 19.2%). 
						  In addition, our ParsingGait achieved the best performance.
					  </i>
				</span>
				</td>
			</tr>
			</table>

			<br>

			<!-- <table>
			<center><h2>(2) Under the cross-domain setting.</h2></center>
			<tr>
				  <td width=400px>
				  <center>
					  <a><img class="rounded" src = "./resources/images/Experiments_cross_domain.png" width="600px"></img></a><br>
				</center>
				</td>
			</tr>
			<tr>
				<td align=center width=720px>
				  <span style="font-size:14px">
					  <i>
						  Results of cross-domain experiments. The method is trained on
						  each source dataset and directly tested on the target datasets.
					  </i>
				</span>
				</td>
			</tr>
			</table> -->

			<br>

			<table>
				<center><h2>(2) Exemplar results.</h2></center>
				<tr>
					  <td width=400px>
					  <center>
						  <a><img class="rounded" src = "./resources/images/experiment_exemplar.png" width="750px"></img></a><br>
					</center>
					</td>
				</tr>
				<tr>
					<td align=center width=720px>
					  <span style="font-size:14px; text-align: justify;">
						  <i>
							Some exemplar results of GaitBase, GaitBase+Parsing, and our ParsingGait. For convenience, we choose the middle
							frame and the frames with four intervals before and after it for visualization. The blue bounding boxes are queries. The green
							bounding boxes are the correctly matched results, while the red bounding boxes are the wrong results. The (a) - (d) represent
							the results under different queries, where the first row of each is the search result of GaitBase, the second row is the result of
							GaitBase+Parsing, and the third row is the result of ParsingGait. (Best viewed in color.)
						  </i>
					</span>
					</td>
				</tr>
				</table>

			
		
					
		</table>
	      <br><br>
		  <hr>


		 <table align=center width=720px>
			<center><h1>Paper</h1></center>
			   <tr>
				 <td align=center><a href=""><img class="layered-paper-big" style="height:160px" src="./resources/images/first_page.png"/></a></td>
				 <td><span style="font-size:14pt">Zheng, Liu, Wang, Wang, Yan, Liu.<br>
				 Parsing is All You Need for Accurate Gait Recognition in the Wild<br>
				 In ACM MM, 2023.<br>
				 (<a href="https://arxiv.org/abs/2204.02569">arXiv</a>)</a>
				 <span style="font-size:4pt"><a href=""><br></a>
				 </span>
				 </td>
				 <td align=center><a href=""><img class="layered-paper-big" style="height:160px" src="./resources/images/first_page_supp.png"/></a></td>
				 <td><span style="font-size:14pt">
				 (<a href="https://arxiv.org/abs/2204.02569">Supplementary</a>)</a>
				 <span style="font-size:4pt"><a href=""><br></a>
				 </span>
				 </td>
			 </tr>
<!--			 <tr>-->
<!--				  <td width=400px>-->
<!--				  <center>-->
<!--					  <a><img class="rounded" src = "./resources/images/TBD.png" width="150px"></img></a><br>-->
<!--				</center>-->
<!--				</td>-->
<!--			</tr>-->
		   </table>
		 
		 <br><br>
		 <hr>

		  <table align=center width=720px>
			<center><h1>Cite</h1></center>
		  <div class="disclaimerbox">
			<!-- <center><h2>How to interpret the results</h2></center> -->

		   <span>
<!--				 <center><span style="font-size:28px"><b>Cite</b></span></center>-->
				<pre style = "font-family:Courier; font-size:14px">
@inproceedings{zheng2023gait3dparsing,
title={Parsing is All You Need for Accurate Gait Recognition in the Wild},
author={Jinkai Zheng, Xinchen Liu, Shuai Wang, Lihao Wang, Chenggang Yan, Wu Liu},
booktitle={ACM International Conference on Multimedia (ACM MM)},
year={2023}
}
				</pre>
<!--			   <pre style = "font-family:Courier; font-size:64px">-->
<!--TBD.-->
<!--				</pre>-->
		  </div>
  		  </table>

			<br><br>
			<hr>
  
		  	
  		  <table align=center width=720px>
  			  <tr>
  	              <td width=400px>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
			  <p style="text-align:justify">
				This work was supported by the National Key Research and Development Program of China under Grant (2020YFB1406604), Beijing
				Nova Program (20220484063), National Nature Science Foundation of China (61931008, U21B2024), "Pioneer", Zhejiang Provincial
				Natural Science Foundation of China (LDT23F01011F01). <br>
				This work was done when Jinkai Zheng was an intern at ID Explore Academy. <br>
			  </p>
</href><a>
			</left>
		</td>
			 </tr>
		</table>

		<br><br>
		<hr>

		<table align=center width=720px>
			<tr>
				<td width=400px>
				  <left>
			<center><h1>Contact</h1></center>
			For further questions and suggestions, please contact Jinkai Zheng (<a href='mailto:zhengjinkai3@hdu.edu.cn'>zhengjinkai3@hdu.edu.cn</a>).
			
			
		</left>
	</td>
		 </tr>
	</table>

		<br><br>

<script>
	
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-1', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 
